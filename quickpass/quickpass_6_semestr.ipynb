{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Программистская часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 1. \n",
    "\n",
    "Напишите черновик игры в жанре RPG. Идеологически: игрок выбирает, будет ли он играть за волшебника или за бойца, а потом выбранным героем сражается с монстрами, набирая очки опыта. Что должно быть технически:\n",
    "\n",
    "- классы волшебника и бойца (можно создать отдельный класс Player и наследоваться от него, но необязательно)\n",
    "- класс монстра (хотя бы один)\n",
    "- класс оружия (тут тоже фантазию не ограничиваю - можно создать абстрактный класс и наследоваться от него, можно сделать классы для меча и для посоха с варьирующими атрибутами)\n",
    "- класс Игра, в котором будут все необходимые методы\n",
    "- все это должно быть разложено по отдельным скриптам .py в папке, класс игры импортируется в файл main.py, и его методы вызываются там. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 2. \n",
    "\n",
    "Дан текст, каждая строка которого является полным или относительным путём к некоторому файлу.\n",
    "Напишите регулярное выражение, которое захватывает:\n",
    "1. директорию, в которой лежит файл;\n",
    "2. только имя файла (без расширения);\n",
    "3. только расширение;\n",
    "При этом:\n",
    "- нужны только файлы, у которых расширение не .bat и не .txt.\n",
    "- пути могут быть как в Unix, так и в Windows формате (https://ru.wikipedia.org/wiki/Путь_к_файлу).\n",
    "- расширение, если оно есть, начинается с точки. Файлы могут быть без расширения вовсе (в этом случае на месте расширения должно стоять None или \"\")\n",
    "- скрытые файлы могут начинаться с точки (например, .bashrc - и это не расширение)\n",
    "- относительный путь может содержать только название файла, в этом случае вместо директории выведите None или \"\"\n",
    "- в остальных случаях директория должна заканчиаться на разделитель директорий. Наприемр, в Unix-системах - \"/\" - это путь к корневой директории.\n",
    "Требуется получить список из кортежей, каждый кортеж содержит извлечённые данные.\n",
    "Используйте флаг VERBOSE, чтобы не запутаться.\n",
    "(Расширение в целом может содержать всё, что угодно, но разделителей директорий не может быть в именах файлах и расширениях. https://en.wikipedia.org/wiki/List_of_filename_extensions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 3. \n",
    "\n",
    "Жизнь.\n",
    "Напишите игру \"Жизнь\".\n",
    "Что это такое - читайте в википедии и здесь: http://www.michurin.net/online-tools/life-game.html\n",
    "Вообще говоря, это не игра в привычном понимании этого слова, а процесс.\n",
    "В простейшем виде достаточно раз в 0.1 секунды выводить на экран обновлённое поле. Для рамочек можно использовать специальные символы для рисования рамочек (найдите в таблице unicode). Пробел - пустая клетка, живая клетка может быть обозначена, например, символом '+'. Начальное поле генерируется случайным образом, \n",
    "вероятность появления жизни в клетке при начальной генерации - должна быть настраиваемым параметром. Размеры поля вводит пользователь при запуске программы. Также должна быть возможность в качестве начальной популяции использовать R-pentomino (http://www.conwaylife.com/wiki/R-pentomino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вообще-то я не в восторге от идеи делать специальный класс под клетку\n",
    "# потому что и атрибутов, и методов тут не то чтобы много, зато начинаются проблемы со ссылками (а было бы так здорово просто хранить 2 больших списка-поля...)\n",
    "# но сделать просто один большой класс Game_of_Life было бы как-то почти бессмысленно (тогда уже проще всё на одних функциях писать), а задача вроде про ООП\n",
    "class Cell:\n",
    "    def __init__(self, is_alive, coords, field):\n",
    "        self.is_alive = is_alive\n",
    "        self.x, self.y = coords\n",
    "        self.field = field\n",
    "        self.neighbors_coords_list = [(self.x + i, self.y + j) for i in range(-1, 2) for j in range(-1, 2) if not i == j == 0 and\n",
    "                          0 <= (self.x + i) < self.field.width and 0 <= (self.y + j) < self.field.heigth]\n",
    "    def __repr__(self):\n",
    "        return str(self.is_alive)\n",
    "\n",
    "    def get_neighbors_sum(self):\n",
    "        return sum(self.field.data[coord[1]][coord[0]].is_alive for coord in self.neighbors_coords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_of_Life:\n",
    "    def __init__(self, width, heigth):\n",
    "        if (heigth <= 0 or width <= 0\n",
    "            or type(heigth) != int or type(width) != int):\n",
    "            raise ValueError('Ширина и высота должны быть целыми положительными числами')\n",
    "        self.width = width\n",
    "        self.heigth = heigth\n",
    "        self.data = []\n",
    "        self.ask_first_intention()\n",
    "    \n",
    "    def __str__(self):\n",
    "        image = '╔' + '═' * (self.width + 2) + '╗\\n'\n",
    "        for y in range(self.heigth):\n",
    "            image += '║ ' + ''.join(['♥' if self.data[y][x].is_alive else '-' for x in range(self.width)]) + ' ║\\n'\n",
    "        image += '╚' + '═' * (self.width + 2) + '╝'\n",
    "        return image\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for y in range(self.heigth):\n",
    "            for x in range(self.width):\n",
    "                yield self.data[y][x].is_alive\n",
    "    # общение\n",
    "    def ask_first_intention(self):\n",
    "        intent = input('''Итак поле готово, нажмите:\n",
    "1 — если хотите зародить жизнь на поле рандомно (надо будет задать вероятность)\n",
    "\n",
    "2 — если хотите поставить на поле R-pentomino\n",
    "\n",
    "3 — если хотите сделать какую-то конкретную клетку живой (вы сможете сделать это и позже\\n''')\n",
    "        if intent == '1':\n",
    "            try:\n",
    "                self.set_random_field(int(input('Введите желаемую вероятность зарождения жизни в процентах — целое число от 0 до 100\\n')))\n",
    "                self.ask_next_intention()\n",
    "            except:\n",
    "                print('Вероятность зарождения жизни должна быть указана в процентах\\n(т. е. быть целым числом от 0 до 100)')\n",
    "                self.ask_first_intention()\n",
    "        elif intent == '2':\n",
    "            self.set_empty_field()\n",
    "            coord = input('Через пробел введите координаты верхней правой (!) клетки вашего R-pentomino\\n').split()\n",
    "            try:\n",
    "                self.set_r_pentomino(int(coord[0]), int(coord[1]))\n",
    "                self.ask_next_intention()\n",
    "            except:\n",
    "                print('Что-то не так с форматом, либо ваш R-pentomino не поместился в поле')\n",
    "                self.ask_first_intention()\n",
    "        elif intent == '3':\n",
    "            coord = input('Через пробел (!) введите координаты клетки, которую хотите сделать живой\\n').split()\n",
    "            try:\n",
    "                self.set_cell(int(coord[0]), int(coord[1]), 1)\n",
    "                self.ask_next_intention()\n",
    "            except:\n",
    "                print('Что-то не так с форматом либо ваша клетка не поместилась в поле')\n",
    "                self.ask_first_intention()\n",
    "        else:\n",
    "            print('Я не понял, давайте ещё раз...')\n",
    "            self.ask_first_intention()\n",
    "    \n",
    "    def ask_next_intention(self):\n",
    "        intent = input('''Теперь можно нажать\n",
    "1 — чтобы запустить жизнь\n",
    "\n",
    "2 — чтобы запустить жизнь, но смотреть на неё пошагово\n",
    "\n",
    "3 — чтобы сделать конкретную клетку живой\n",
    "\n",
    "4 — чтобы убить конкретную клетку\n",
    "\n",
    "5 — чтобы просто полюбоваться полем :)\\n''')\n",
    "        # можно было, конечно придумать специальную функцию, а не писать if-else\n",
    "        # но так как разные пути требуют разной цепочки вводов в дальнейшем\n",
    "        # не сильно стало бы красивее от такой функции\n",
    "        if intent == '1':\n",
    "                self.draw()\n",
    "        elif intent == '2':\n",
    "            self.draw_by_steps()\n",
    "        elif intent == '3':\n",
    "            coord = input('Через пробел (!) введите координаты клетки, которую хотите сделать живой\\n').split()\n",
    "            try:\n",
    "                self.set_cell(int(coord[0]), int(coord[1]), 1)\n",
    "            except:\n",
    "                print('Что-то не так с форматом либо ваша клетка не поместилась в поле')\n",
    "            self.ask_next_intention()\n",
    "        elif intent == '4':\n",
    "            coord = input('Через пробел (!) введите координаты клетки, которую хотите убить\\n').split()\n",
    "            try:\n",
    "                self.set_cell(int(coord[0]), int(coord[1]), 0)\n",
    "            except:\n",
    "                print('Что-то не так с форматом либо ваша клетка не поместилась в поле')\n",
    "            self.ask_next_intention()\n",
    "        elif intent == '5':\n",
    "            print(self)\n",
    "            self.ask_next_intention()\n",
    "        else:\n",
    "            print('Я не понял давайте ещё раз...')\n",
    "            self.ask_next_intention()\n",
    "\n",
    "    # настройка поля\n",
    "    def set_empty_field(self):\n",
    "        self.data = []\n",
    "        for y in range(self.heigth):\n",
    "            line = []\n",
    "            for x in range(self.width):\n",
    "                line.append(Cell(0, (x, y), self))\n",
    "            self.data.append(line)\n",
    "\n",
    "    def set_random_field(self, livebility):\n",
    "        self.data = []\n",
    "        if type(livebility) != int or not (0 <= livebility < 100):\n",
    "            raise ValueError('Вероятность зарождения жизни указана не в процентах')\n",
    "        for y in range(self.heigth):\n",
    "            line = []\n",
    "            for x in range(self.width):\n",
    "                line.append(Cell(int(randint(0, 100) <= livebility), (x, y), self))\n",
    "            self.data.append(line)\n",
    "\n",
    "    def set_r_pentomino(self, x, y):\n",
    "        cells = [(x, y), (x - 1, y), (x - 1, y + 1),\n",
    "                (x - 2, y + 1), (x - 1, y + 2)]\n",
    "        for coord in cells:\n",
    "            if not (0 <= coord[0] < self.width) or not (0 <= coord[1] < self.heigth):\n",
    "                raise ValueError('R-pentomino не поместился в поле :(')\n",
    "            self.data[coord[1]][coord[0]].is_alive = 1\n",
    "        print(self)\n",
    "    \n",
    "    def set_cell(self, x, y, is_alive):\n",
    "        if not self.data: self.set_empty_field()\n",
    "        if type(is_alive) != int or not (is_alive in [0, 1]):\n",
    "            raise ValueError('Клетка либо жива, либо мертва')        \n",
    "        if type(x) != int or not (0 <= x < self.width):\n",
    "            raise ValueError('x не поместился в поле :(')\n",
    "        if type(y) != int or not (0 <= y < self.heigth):\n",
    "            raise ValueError('y не поместился в поле :(')\n",
    "        self.data[y][x].is_alive = is_alive\n",
    "    \n",
    "    # движ    \n",
    "    def upd(self):\n",
    "        # вот тут вскрывается неоптимальность класса Cell\n",
    "        # я воспользовалась модулем, если так нельзя, то пусть я просто ещё раз цикл в цикле добавила :(\n",
    "        prev_state = deepcopy(self.data)\n",
    "        # я задумалась о том, чтобы сделать что-то с map, как-то так чтобы только уже живые и их соседи проверялись (ну типа оптимизация)\n",
    "        # но что-то так и не придумала ничего\n",
    "        # поэтому довольно медленно оно работает к сожалению\n",
    "        for y in range(self.heigth):\n",
    "            for x in range(self.width):\n",
    "                if (prev_state[y][x].get_neighbors_sum() == 3) or (prev_state[y][x].get_neighbors_sum() + prev_state[y][x].is_alive == 3):\n",
    "                    self.data[y][x].is_alive = 1\n",
    "                else:\n",
    "                    self.data[y][x].is_alive = 0\n",
    "\n",
    "    def draw(self):\n",
    "        while sum(self):\n",
    "            clear_output(wait = True)\n",
    "            print(self)\n",
    "            self.upd()\n",
    "        clear_output(wait = True)\n",
    "        print(self)\n",
    "    \n",
    "    def draw_by_steps(self):\n",
    "        input_text = ':)'\n",
    "        while input_text != '0':\n",
    "            clear_output(wait = True)\n",
    "            print(self)\n",
    "            self.upd()\n",
    "            input_text = input('Введите 0, если хотите остановиться\\nВведите что-нибудь другое, если хотите продолжать\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Game_of_Life(40, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Лингвистическая часть\n",
    "\n",
    "Для выполнения этих заданий выберите два любых достаточно длинных текста (.txt) на русском и на любом другом (для которого есть парсеры) языке; если возьмете текст и его перевод, будет отлично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 4. \n",
    "\n",
    "Просмотрите оба выбранных текста. Удостоверьтесь, что тексты чистые, если же в них есть какой-то мусор: хештеги, затесавшиеся при OCR символы и подобное, почистите с помощью регулярных выражений. \n",
    "\n",
    "Проведите первичный статистический анализ: разбейте тексты на предложения и на токены, посчитайте относительное количество того и другого, сопоставьте. Если ваши тексты параллельные, какой длиннее? В каком тексте средняя длина предложения больше? Почему? В каком тексте выше лексическое разнообразие? \n",
    "\n",
    "Таким образом, вам необходимо узнать следующие вещи:\n",
    "\n",
    "- количество предложений (относительное и абсолютное)\n",
    "- количество токенов (относительное и абсолютное)\n",
    "- средняя длина предложения (среднее количество слов в предложении)\n",
    "- соотношение \"уникальные токены / все токены\"\n",
    "- (опционально) соотношение знаков пунктуации и слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mojsrpski.org/reading/parallel — источник\n",
    "# тексты — \"Мост на Дрине\" (1 глава) параллельно на русском и сербском\n",
    "with open('most_na_drine_rus.txt', 'r', encoding='utf-8') as file:\n",
    "    rus_text = file.read() # UDPipe с моделью синтагруса хуже разбиравет \"ёлочки\", чем кавычки\n",
    "with open('most_na_drine_srb.txt', 'r', encoding='utf-8') as file:\n",
    "    srb_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество предложений можно, конечно, было попытаться посчитать регуляркой\n",
    "# что-то типа:\n",
    "import re\n",
    "pattern = r'\\s?(\\(?(?:[»\\\"].+?[«\\\"]|.)+?(?:(?<!\\s\\w)\\.|\\?|!){1,3}\\)?)\\s?'\n",
    "# sents = re.findall(pattern, rus_text.replace('\\n', ' '), flags=(re.MULTILINE))\n",
    "# sents = re.findall(pattern, srb_text.replace('\\n', ' '), flags=(re.MULTILINE))\n",
    "# print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# но я сразу возьму UDPipe (запрета на UDPipe до 5-ого задания я не нашла (: )\n",
    "from conllu import parse\n",
    "with open('processed_rus.conllu', 'r', encoding='utf-8') as rus_conllu:\n",
    "    parsed_rus = parse(rus_conllu.read())\n",
    "\n",
    "with open('processed_srb.conllu', 'r', encoding='utf-8') as srb_conllu:\n",
    "    parsed_srb = parse(srb_conllu.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # потому что регулярка не идеальна, и nltk.sent_tokenize() тоже так себе работает (если раскомментировать, будет видно, почему)\n",
    "# import nltk\n",
    "# # nltk.download('punkt')\n",
    "# sentences = nltk.sent_tokenize(srb_text)\n",
    "# for i in range(len(parsed_srb)):\n",
    "#     if parsed_srb[i].metadata['text'] != sents[i]:\n",
    "#         print('PREV:\\n' + parsed_srb[i - 1].metadata['text'] + '\\n\\n' + sentences[i - 1] + '\\n\\n' + sents[i - 1] + '\\n\\n\\n')\n",
    "#         print('CURR:\\n' + parsed_srb[i].metadata['text'] + \"\\n\\n\" + sentences[i]+ '\\n\\n' + sents[i] + '\\n\\n\\n')\n",
    "#         print('NEXT:\\n' + parsed_srb[i+1].metadata['text'] + \"\\n\\n\" + sentences[i+1]+ '\\n\\n' + sents[i+1] + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы всякие относительные вещи считать\n",
    "def percent(numerator, divisor):\n",
    "    return round(numerator * 100 / divisor, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "количество предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно было взять больше одной главы, но там довольно точный перевод, поэтому большой разницы там бы всё равно не было\n",
    "# (я не взяла, потому что в сербском тексте вручную расставлены переносы, ручками убирать их я не хочу,\n",
    "# а настолько хорошие регулярки писать не умею)\n",
    "print(f'{len(parsed_srb)} предложений — в оригинальном тексте на сербском')\n",
    "print(f'{len(parsed_rus)} предложений — в переводе на русский')\n",
    "print(f'{percent(len(parsed_rus), len(parsed_srb))}% — количества предложений в оригинале количество предложений в переводе на русский составляет')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "количество токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rus_tokens = sum(len(sent) for sent in parsed_rus)\n",
    "count_srb_tokens = sum(len(sent) for sent in parsed_srb)\n",
    "print(f'{count_srb_tokens} токенов — в оригинальном тексте на сербском')\n",
    "print(f'{count_rus_tokens} токенов — в переводе на русский')\n",
    "print(f'{percent(count_rus_tokens, count_srb_tokens)}% от количества токенов на сербском составляет количество токенов на русском')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "длина предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_sent(sent):\n",
    "    return sum(map(lambda token: token.get('upos') != 'PUNCT', sent))\n",
    "\n",
    "def mean_len(parsed_text):\n",
    "    return round(sum(len_sent(sent) for sent in parsed_text) / len(parsed_text), 3)\n",
    "\n",
    "print(f'{mean_len(parsed_srb)} — средняя длина сербского предложения')\n",
    "print(f'{mean_len(parsed_rus)} — средняя длина русского предложения')\n",
    "print(f'{percent(mean_len(parsed_rus), mean_len(parsed_srb))}% от средней длины сербского предложенияя составляет средняя длина русского')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "уникальные токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uniq_tokens(parsed_text):\n",
    "    res = set()\n",
    "    for sent in parsed_text:\n",
    "        res.update(set(token.get('form') for token in sent))\n",
    "    return res\n",
    "\n",
    "# здорово, какая большая разница!\n",
    "print(f'{percent(len(count_uniq_tokens(parsed_srb)), sum(len(sent) for sent in parsed_srb))}% процент уникальных токенов в сербском тексте')\n",
    "print(f'{percent(len(count_uniq_tokens(parsed_rus)), sum(len(sent) for sent in parsed_rus))}% процент уникальных токенов в русском тексте')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "знаки препинания на слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_puncts_words(parsed_text):\n",
    "    punct_counter = 0\n",
    "    word_counter = 0\n",
    "    for sent in parsed_text:\n",
    "        punct_in_sent = sum(map(lambda token: token.get('upos') == 'PUNCT', sent))\n",
    "        punct_counter += punct_in_sent\n",
    "        word_counter +=  (len(sent) - punct_in_sent)\n",
    "    return punct_counter, word_counter\n",
    "punct_rus, word_rus = count_puncts_words(parsed_rus)\n",
    "punct_srb, word_srb = count_puncts_words(parsed_srb)\n",
    "\n",
    "print(f'В сербском тексте:\\n\\t{word_srb}\\tслов ({percent(word_srb,(word_srb + punct_srb))}% токенов)\\n\\t{punct_srb}\\tзнаков препинания ({percent(punct_srb, (word_srb + punct_srb))}% токенов)\\n\\t{round(word_srb / punct_srb, 2)}\\tслов на 1 знак препинания')\n",
    "print(f'В русском тексте:\\n\\t{word_rus}\\tслов ({percent(word_rus,(word_rus + punct_rus))}% токенов)\\n\\t{punct_rus}\\tзнаков препинания ({percent(punct_rus, (word_rus + punct_rus))}% токенов)\\n\\t{round(word_rus / punct_rus, 2)}\\tслов на 1 знак препинания')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 5. \n",
    "\n",
    "Сделайте морфосинтаксические разборы ваших текстов в формате UD, запишите .conllu-файлы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно скачать (https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4923#) модельки\n",
    "# через ufal.udpipe к ним обращаться локально\n",
    "# нашла для русского отдельно какую-то модель (не очень хорошую): https://github.com/ancatmara/data-science-nlp/raw/master/data/russian-ud-2.0-170801.udpipe, для сербского не нашла\n",
    "# в полной версии это полгигабайта, и менять я их не собираюсь, зачем тогда скачивать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы показать, что я умею:\n",
    "from ufal.udpipe import Model\n",
    "from ufal.udpipe import Pipeline\n",
    "import wget\n",
    "\n",
    "wget.download('https://github.com/ancatmara/data-science-nlp/raw/master/data/russian-ud-2.0-170801.udpipe')\n",
    "model = Model.load('russian-ud-2.0-170801.udpipe')\n",
    "pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, Pipeline.DEFAULT)\n",
    "ex_text = 'Я нашла для русского отдельно какую-то модель (не очень хорошую)'\n",
    "print(pipeline.process(ex_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно использовать API, но так нельзя парсить крупные тексты\n",
    "# пришлось бы отправлять по 1-2 предложения\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_conllu_with_api(text, model):\n",
    "    url = 'http://lindat.mff.cuni.cz/services/udpipe/api/process'\n",
    "    params = {\n",
    "        'tokenizer': '', 'tagger': '', 'parser': '', \n",
    "        'model': model,\n",
    "        'input': 'generic_tokenizer',\n",
    "        'data': text}\n",
    "    response = requests.get(url, params)\n",
    "    return response.json()['result']\n",
    "\n",
    "ex_text_rus = 'можно использовать API, но так нельзя парсить крупные тексты'\n",
    "ex_text_srb = 'moguće je koristiti API, ali tako ne možete strugati velike textove'\n",
    "print(get_conllu_with_api(ex_text_rus, 'russian-syntagrus-ud-2.10-220711'))\n",
    "print(get_conllu_with_api(ex_text_srb, 'serbian-set-ud-2.10-220711'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поэтому просто ручками вставила текст сюда: http://lindat.mff.cuni.cz/services/udpipe/run.php\n",
    "# использовала 'russian-syntagrus-ud-2.10-220711' для русского b 'serbian-set-ud-2.10-220711' для сербского\n",
    "# и сохранила output-файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если нужно было написать парсер conllu-файлов самому, у меня уже есть, правда, довольно плохой, старый код, который это делает\n",
    "from conllu import parse\n",
    "with open('processed_rus.conllu', 'r', encoding='utf-8') as rus_conllu:\n",
    "    parsed_rus = parse(rus_conllu.read())\n",
    "\n",
    "with open('processed_srb.conllu', 'r', encoding='utf-8') as srb_conllu:\n",
    "    parsed_srb = parse(srb_conllu.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 6. \n",
    "\n",
    "Посчитайте статистику по частям речи, сопоставьте: можно напечатать две таблички с процентами по частям речи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_param_stat(parsed_text, param, *upos_exceptions):\n",
    "    res = {}\n",
    "    for sentence in parsed_text:\n",
    "        for token in sentence:\n",
    "            if not token.get('upos') in upos_exceptions:\n",
    "                if token.get(param) not in res:\n",
    "                    res[token.get(param)] = 0\n",
    "                res[token.get(param)] += 1\n",
    "    return res\n",
    "\n",
    "def pos_stat_in_table(pos_dict):\n",
    "    all_tokens_sum = sum(pos_dict[pos] for pos in pos_dict)\n",
    "    res = 'Часть речи\\tКоличество\\tВ процентах\\n\\n'\n",
    "    for pos in sorted(pos_dict.keys()):\n",
    "        res += f'{pos}\\t\\t{pos_dict[pos]}\\t\\t{percent(pos_dict[pos], all_tokens_sum)}%\\n'\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_pos_rus = tokens_param_stat(parsed_rus, 'upos', 'PUNCT')\n",
    "print('Текст на русском:\\n\\n' + pos_stat_in_table(counted_pos_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# есть загадочная строка:\n",
    "# 32\ttarih\ttar\tX\tNcmsn\tForeign=Yes\t30\tappos\t_\tTokenRange=4702:470\n",
    "# не знаю, что это и верно ли оно распознанно (вряд ли), и такое слово всего одно, поэтому просто убрала его из подсчётов\n",
    "counted_pos_srb = tokens_param_stat(parsed_srb, 'upos', 'PUNCT', 'X')\n",
    "print('Текст на сербском:\\n\\n' + pos_stat_in_table(counted_pos_srb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 7. \n",
    "\n",
    "Посчитайте, какое соотношение токенов по частям речи имеет совпадающие со словоформой леммы (т.е., в скольких случаях токены с частью речи VERB, например, имели словарную форму: и сам токен, и лемма одинаковые). Что вы можете сказать о выбранных вами языках на основании этих данных? Ожидаются две таблички с процентами несовпадающих по лемме и токену слов для каждой части речи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_forms_counter(parsed_text, *exceptions):\n",
    "    res = {}\n",
    "    for sentence in parsed_text:\n",
    "        for token in sentence:\n",
    "            if not token.get('upos') in exceptions:\n",
    "                if token.get('upos') not in res.keys(): \n",
    "                    res[token.get('upos')] = 0\n",
    "                res[token.get('upos')] += int(token.get('form') == token.get('lemma'))\n",
    "    return res\n",
    "\n",
    "def dict_forms_table(dict_forms_pos_dict, pos_dict):\n",
    "    res = 'Часть речи\\tФорм, совпадающих с леммами\\tВсего форм\\tПроцент совпадения\\n\\n'\n",
    "    for pos in sorted(pos_dict.keys()):\n",
    "        res += f'{pos}\\t\\t{dict_forms_pos_dict[pos]}\\t\\t\\t\\t{pos_dict[pos]}\\t\\t{round(dict_forms_pos_dict[pos] * 100 / pos_dict[pos], 2)}%\\n'\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_dict_forms_rus = dict_forms_counter(parsed_rus, 'PUNCT')\n",
    "print(dict_forms_table(counted_dict_forms_rus, counted_pos_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_dict_forms_srb = dict_forms_counter(parsed_srb, 'PUNCT', 'X')\n",
    "print(dict_forms_table(counted_dict_forms_srb, counted_pos_srb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Что я могу сказать о выбранных мной языках на основании этих данных:*\n",
    "\n",
    "*В этих текстах русский и сербский примерно одинаково синтетические: \"более изменяемые\" части речи \"компенсируют\" менее изменяемые, тут дела по-разному у русского и сербского.*\n",
    "\n",
    "*Были бы это не 2 славянских языка, а, например, русский и корейский, разница была бы больше, полагаю.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 8. \n",
    "\n",
    "Посчитайте медианную длину предложения для ваших текстов (медиана - это если взять все длины всех ваших предложений, упорядочить их от маленького к большому и выбрать то число, которое оказалось посередине, а если чисел четное количество, то взять среднее арифметическое двух чисел посередине. Например, если у вас пять предложений длинами 1, 2, 6, 7, 8, то медиана - 6, а если шесть предложений длинами 1, 1, 7, 9, 10, 11, то медиана - (7 + 9) / 2 = 8). Возьмите любые два предложения (одно русское и второе на другом языке) и постройте для них деревья зависимостей. Изучите связи зависимостей (deprel) и вершины: согласны ли вы с разбором?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(nums):\n",
    "    nums.sort()\n",
    "    middle_index = len(nums) // 2\n",
    "    if len(nums) % 2: return sorted(nums)[middle_index]\n",
    "    return round((nums[middle_index - 1] + nums[middle_index]) / 2, 2)\n",
    "\n",
    "# уже было, но повторю\n",
    "def len_sent(sent):\n",
    "    return sum(map(lambda token: token.get('upos') != 'PUNCT', sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_median([len_sent(sent) for sent in parsed_srb])\n",
    "print(f'{get_median([len_sent(sent) for sent in parsed_srb])} — средняя длина сербского предложения')\n",
    "print(f'{get_median([len_sent(sent) for sent in parsed_rus])} — средняя длина русского предложения')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "деревья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Node, Tree\n",
    "def add_one(token, sent, tree):\n",
    "    if token.get('head') == 0 and not token.get('id') in tree.nodes:\n",
    "        tree.create_node(str(token.get('id')) + ': ' + token.get('upos') + ' ' + token.get('form'), token.get('id'))\n",
    "    elif not token.get('id') in tree.nodes and token.get('upos') != 'PUNCT':\n",
    "        if not token.get('head') in tree.nodes:\n",
    "            add_one(sent[token.get('head') - 1], sent, tree)\n",
    "        tree.create_node(str(token.get('id')) + ': ' + token.get('upos') + ' ' + token.get('form').lower() + ' (' + token.get('deprel') + ')',\n",
    "                         token.get('id'), parent=token.get('head'))\n",
    "def draw_tree(sent):\n",
    "    tree = Tree()\n",
    "    for token in sent:\n",
    "        add_one(token, sent, tree)\n",
    "    tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(parsed_rus[3])\n",
    "draw_tree(parsed_srb[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Согласна ли я с разбором*\n",
    "*Не очень:*\n",
    "- *в нём участвуют знаки препинания*\n",
    "- *вместо привычной синтаксической вершины предложной группы — вершина подчинённой именной, получается* **PP \\[ NP \\[ AdjP \\] \\]** *вместо* **NP \\[ \\[ PP \\] \\[ AdjP \\] \\]** *(а в \"друг к другу\" даже не подчинённой, но реципрокальные местоимения — это вообще не спортивно, если мы про синтаксис)*\n",
    "- *подчинение придаточных: по идее должно быть 16 → 18 → 21 или 16 → 21 → 18 (это спорный момент), но никак не 21 → 16 + 21 → 18*\n",
    "- *не то чтобы я знала сербский, но то, что отрицательная частица зависит от существительного:* **(deprel:advmod) form:ne lemma:ne upos:PART** *точно неправильно*\n",
    "\n",
    "*Я не знаю почему так устроено, зато знаю, что это не баг, а фича, и оно в принципе так работает*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 9. \n",
    "\n",
    "Посчитайте частотные списки токенов для каждой категории связей зависимостей (т.е., нужно выделить все токены в тексте, которые получали, например, ярлык amod, и посчитать их частоты). Выведите по первые три самых частотных токена для каждой категории (punct можно не выводить). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprel_stat_in_table(deprel_dict):\n",
    "    all_tokens_sum = sum(deprel_dict[kind] for kind in deprel_dict)\n",
    "    res = 'Тип зависимости\\tКоличество\\tВ процентах\\n\\n'\n",
    "    for kind in sorted(deprel_dict.keys()):\n",
    "        kind_str = kind.ljust(8, ' ')\n",
    "        res += f'{kind_str}\\t{deprel_dict[kind]}\\t\\t{percent(deprel_dict[kind], all_tokens_sum)}%\\n'\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprel_stat_rus = tokens_param_stat(parsed_rus, 'deprel', 'PUNCT')\n",
    "print('В тексте на русском:\\n\\n' + deprel_stat_in_table(deprel_stat_rus))\n",
    "\n",
    "deprel_stat_srb = tokens_param_stat(parsed_srb, 'deprel', 'PUNCT')\n",
    "print('В тексте на сербском:\\n\\n' + deprel_stat_in_table(deprel_stat_srb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 10. \n",
    "\n",
    "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
    "\n",
    "- составить список таких предлогов (РГ-80 вам в помощь)\n",
    "- взять достаточно большой текст (можно большое художественное произведение)\n",
    "- сделать морфоразбор этого текста (лучше не pymorphy)\n",
    "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
    "\n",
    "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести).\n",
    "\n",
    "Если будете использовать RNNMorph, возможно, понадобится регулярное выражение и немного терпения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*составить список таких предлогов*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions = {'между': {'твор', 'род'},\n",
    "                'меж': {'твор', 'род'},\n",
    "                'с': {'род', 'вин', 'твор'},\n",
    "                'по': {'дат', 'вин', 'местн', 'пр'},\n",
    "                'в': {'вин', 'местн', 'пр'},\n",
    "                'за': {'вин', 'твор'},\n",
    "                'на': {'вин', 'местн', 'пр'},\n",
    "                'о': {'вин', 'местн', 'пр'},\n",
    "                'под': {'вин', 'твор'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*взять достаточно большой текст*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('piramidi.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text_justified_spaces = re.sub(r'[\\s\"\\(\\)]+', ' ', text) #кавычки и скобки убираем, потому что потом может идти слово в нужном падеже\n",
    "\n",
    "with open('text_to_lem.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text_justified_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*сделать морфоразбор этого текста*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mystem -n -c -i -d --format json text_to_lem.txt lem_res.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lem_res.json', 'r', encoding='utf-8') as f:\n",
    "    lem_dict = [json.loads(line) for line in f.readlines() if line != '{\"text\":\" \"}\\n']\n",
    "    lemmatized = list(filter(lambda x: x != '{\"text\":\" \"}\\n', f.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если просто смотреть следующее слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# варианты падежей, которые в принципе подчиняются предлогам (т. е. кроме им, парт, зват)\n",
    "def get_possible_cases(token_dict):\n",
    "    res = set()\n",
    "    for var in token_dict['analysis']:\n",
    "        var_case = re.findall('[=,](род|вин|дат|твор|местн|пр)(?:,|$)', var['gr'])\n",
    "        # всё-таки руководствуемся РГ-80, и считаем, что местного не существует, но при желании replace можно просто убрать и статка чуть поменяется\n",
    "        if var_case: res.add(var_case[0].replace('местн', 'пр'))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "for i in range(len(lem_dict)):\n",
    "    if ('analysis' in lem_dict[i] and len(lem_dict[i]['analysis'])\n",
    "        and lem_dict[i]['analysis'][0]['lex'] in prepositions):\n",
    "        # если следуюющий токен без анализа\n",
    "        if not 'analysis' in lem_dict[i + 1] or not len(lem_dict[i + 1]['analysis']):\n",
    "                continue\n",
    "        next_word_case = get_possible_cases(lem_dict[i + 1])\n",
    "        if next_word_case:\n",
    "            next_word_case_str = ' || '.join(next_word_case)\n",
    "            if not next_word_case_str in res: res[next_word_case_str] = 0\n",
    "            res[next_word_case_str] += 1\n",
    "\n",
    "for case in sorted(res.keys()):\n",
    "    print(case.ljust(33) + str(res[case]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если заморачиваться — пытаться учитывать синкретизм, родительный падеж влево (*'в его глазах'*), числа и всякое такое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить часть речи\n",
    "def get_pos(token_dict):\n",
    "    if not 'analysis' in token_dict and token_dict['text'].isdigit():\n",
    "        return 'DIGIT'\n",
    "    if not 'analysis' in token_dict or not len(token_dict['analysis']):\n",
    "        return 'no_pos'\n",
    "    res = token_dict['analysis'][0]['gr'][:token_dict['analysis'][0]['gr'].find('=')]\n",
    "    if res.count(','): return res[:res.find(',')]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# варианты падежей в пересечении с теми, которые допускает предлог\n",
    "def get_possible_cases(token_dict, prepos_cases):\n",
    "    res = set()\n",
    "    for var in token_dict['analysis']:\n",
    "        var_case = re.findall(r'[=,](род|вин|дат|твор|местн|пр)(?:,|$)', var['gr'])\n",
    "        # всё-таки руководствуемся РГ-80, и считаем, что местного не существует, но при желании replace можно просто убрать и статка чуть поменяется\n",
    "        if var_case: res.add(var_case[0].replace('местн', 'пр'))\n",
    "    return res & prepos_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# какие встречаются части речи после предлогов\n",
    "pos_list = set()\n",
    "for i in range(len(lem_dict)):\n",
    "    if ('analysis' in lem_dict[i] and len(lem_dict[i]['analysis'])\n",
    "        and lem_dict[i]['analysis'][0]['lex'] in prepositions):\n",
    "        next_word_pos = get_pos(lem_dict[i + 1])\n",
    "        if next_word_pos not in pos_list: pos_list.add(next_word_pos)\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# те, для которых следующее слово никогда не смотрим\n",
    "check_case = {'NUM', 'S', 'SPRO'}\n",
    "# те, для которых следующее слово смотрим всегда\n",
    "check_next = {'ADV', 'ADVPRO', 'DIGIT'}\n",
    "# те, для которых следующее слово смотрим, только если падеж этого не однозначен\n",
    "check_next_if_many = {'A', 'ANUM', 'APRO', 'V'}\n",
    "# те, которые не войдут в анализ вообще — мусор или слишком сложно\n",
    "skip = {'CONJ', 'PART', 'PR', 'no_pos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_res(case_set):\n",
    "    if case_set:\n",
    "        res_key = ' || '.join(case_set)\n",
    "        if not res_key in res: res[res_key] = 0\n",
    "        res[res_key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так убого, потому что нормальное ограничение для рекурсии не придумала\n",
    "# тоже не совершенный алгоритм: для примера \"на совершенно неприличного вида сливу\" в копилку \"на\" пойдёт \"вин\", но таких случаев ничтожно мало, вроде бы\n",
    "res = dict()\n",
    "for i in range(len(lem_dict)):\n",
    "    if ('analysis' in lem_dict[i] and len(lem_dict[i]['analysis'])\n",
    "        and lem_dict[i]['analysis'][0]['lex'] in prepositions):\n",
    "        index_to_check = i + 1\n",
    "        prepos_cases = prepositions[lem_dict[i]['analysis'][0]['lex']]\n",
    "        next_word_pos = get_pos(lem_dict[index_to_check])\n",
    "        if next_word_pos in skip:\n",
    "            continue\n",
    "        if next_word_pos in check_case:\n",
    "            update_res(get_possible_cases(lem_dict[index_to_check], prepos_cases))\n",
    "        elif next_word_pos in check_next:\n",
    "            index_to_check += 1\n",
    "            next_word_pos = get_pos(lem_dict[index_to_check])\n",
    "            if next_word_pos in skip: continue\n",
    "            possible_cases = get_possible_cases(lem_dict[index_to_check], prepos_cases)\n",
    "            if len(possible_cases) == 1:\n",
    "                update_res(possible_cases)\n",
    "            elif (get_pos(lem_dict[index_to_check]) in check_next_if_many\n",
    "                and get_pos(lem_dict[index_to_check + 1]) not in skip.union(check_next)):\n",
    "                update_res(possible_cases & get_possible_cases(lem_dict[index_to_check + 1], prepos_cases))\n",
    "        else:\n",
    "            possible_cases = get_possible_cases(lem_dict[index_to_check], prepos_cases) \n",
    "            if len(possible_cases) == 1 or get_pos(lem_dict[index_to_check + 1]) in skip.union(check_next):\n",
    "                update_res(possible_cases)\n",
    "            else:\n",
    "                if (possible_cases & get_possible_cases(lem_dict[index_to_check + 1], prepos_cases)):\n",
    "                    update_res(possible_cases & get_possible_cases(lem_dict[index_to_check + 1], prepos_cases))\n",
    "                else:\n",
    "                    update_res(possible_cases)\n",
    "for case in sorted(res.keys()):\n",
    "    print(case.ljust(20) + str(res[case]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
